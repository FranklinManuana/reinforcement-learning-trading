# Reinforcement Learning Trading ğŸ“ˆ

This project implements a Deep Q-Learning (DQN) agent for financial trading using a custom reinforcement learning environment. It is designed to simulate and train a trading agent using historical financial data, neural networks, and experience replay.

---

## ğŸ” Overview

This project offers:

- ğŸ”„ A modular design for ease of use and repurposing
- Integration with real and historical market data sources
- Utilities for plotting, evaluation, logging, and analysis

---

## ğŸ“‚ Repository Structure

```
reinforcement-learning-trading/
â”‚
â”œâ”€â”€ output/                          # Output files generated during training/evaluation
â”‚   â”œâ”€â”€ figures/                     # Plots and visualizations
â”‚   â””â”€â”€ trained_models/             # Saved model weights/checkpoints
â”‚
â”œâ”€â”€ src/                             # Core source code
â”‚   â”œâ”€â”€ modules/                     # Modular Python files
â”‚   â”‚   â”œâ”€â”€ config.py               # Configuration and seed setup
â”‚   â”‚   â”œâ”€â”€ DQN_model.py           # Deep Q-Network (DQN) model architecture
â”‚   â”‚   â”œâ”€â”€ environment.py         # Trading environment logic
â”‚   â”‚   â”œâ”€â”€ evaluate.py            # Evaluation logic for trained agents
â”‚   â”‚   â”œâ”€â”€ figures.py             # Plotting utilities (e.g., loss, RÂ²)
â”‚   â”‚   â”œâ”€â”€ model_settings.py      # Hyperparameters and training setup
â”‚   â”‚   â”œâ”€â”€ replay.py              # Experience replay buffer implementation
â”‚   â”‚   â”œâ”€â”€ stock_data.py          # Stock data loading and preprocessing
â”‚   â”‚   â””â”€â”€ train.py               # Model training loop
â”‚
â”œâ”€â”€ notes/                           # (Optional) Notes or experiment logs
â”‚
â”œâ”€â”€ main.ipynb                       # Entry-point Jupyter notebook for running the model
â”œâ”€â”€ README.md                        # Project overview and documentation
```
---

## Phases

### 1. Trading Environment
A custom trading environment built to simulate portfolio management tasks. It tracks:
- Balance
- Shares held
- Net worth
- Actions: buy, hold, sell

### 2. Network Architecture
A deep neural network (DQN variant) is constructed using PyTorch. The architecture includes:
- Fully connected layers
- LeakyReLU activations
- Q-value outputs for trading actions

### 3. Replay Buffer
An experience replay class that stores past transitions (state, action, reward, next state) to sample mini-batches during training. This helps stabilize and decorrelate learning.

### 4. Parameters / Settings
Hyperparameters include:
- Learning rate
- Discount factor (gamma)
- Epsilon-greedy parameters
- Batch size
- Episode length

### 5. Data Loading
Historical price data is loaded and preprocessed to be used in the environment. Supports:
- Multiple tickers
- Train/test splits (e.g., by year)

### 6. Training
The DQN agent is trained over multiple episodes with:
- Epsilon-greedy exploration
- Gradient updates using MSE loss
- Target Q-value calculation

### 7. Evaluation
Post-training evaluation includes:
- Performance visualization (net worth over time)
- RÂ² and loss logging per iteration
---

## ğŸ¯ Features

- **Modular environments**: Swap between stock, crypto, or multi-asset paper trading setups.
- **Multiple RL algorithms**: DQN, Double DQN, PPO, SAC, etc. â€” easily extendable.
- **Metrics & plots**: Analyze returns, Sharpe, drawdowns, and compare against baselines.
- **Real-time trading**: Paper/live trading mode for demo or backtesting.

---

## ğŸ”§ Configuration

Edit `model_settings.py`  `config.py` to adjust settings such as:
`model_settings.py`
- Data source / preprocessing rules
- Environment parameters (e.g., window size, action space)
- RL agent hyperparameters (learning rate, gamma, etc.)
- Logging & checkpoint directories
`config.py`

---

## ğŸ“Š Logs & Output

- plots, metrics, and models are saved to outputğŸ“‚ 
- `figures.py` use to visually display:
  - Training loss & episode returns
  - Portfolio Networth
---

---

## âœ… Requirements

- Python 3.9+
- Core libraries: `torch`, `numpy`, `pandas`, `matplotlib`
- RL frameworks: optional

---

## ğŸ“„ Best Practices

- ğŸ§  Use **non-leaky feature windows** to avoid lookahead bias
- ğŸ’¸ Include real-world factors: transaction costs, slippage, and order fill assumptions
- ğŸ—•ï¸ **Train/test split** should be chronological to mimic real deployment
- ğŸ“ Maintain **random seed control** for reproducible results

---

## ğŸ“š References

   ğŸ“š
- [yFinance Documentation](https://ranaroussi.github.io/yfinance/)
- [TensorFlow Introduction to DQN](https://www.tensorflow.org/agents/tutorials/0_intro_rl)
- [Medium Article on Deep Q-Learning (DQN)](https://medium.com/@samina.amin/deep-q-learning-dqn-71c109586bae) by Samina Amin 2024
- [Medium Article on Deep Q-Network(DQN)](https://medium.com/@shruti.dhumne/deep-q-network-dqn-90e1a8799871) by Shruti Dhumme

   
   ğŸ’»
- [Making Neural Network From Scratch](https://www.youtube.com/watch?v=w8yWXqWQYmU&t=1325s)

---

## ğŸ›  Contribution & License

You're welcome to open issues, suggest algorithms, or add new environments. Pull requests are highly appreciated!

**License**: [Your Choice - MIT / Apache 2.0 / etc.]

---

### ğŸš€ What's next?
- ğŸ’¸ Include real-world factors: transaction costs, slippage, and order fill assumptions
- Add risk-sensitive agents like **SAC** or **PPO**
- Integrate **live data APIs** (e.g. Alpaca, Binance)
- Add **multi-asset portfolio optimization**
